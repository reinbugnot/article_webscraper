{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb6bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb85db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(url):\n",
    "    \"\"\"Return the source code for the provided URL. \n",
    "\n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd55a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google(query, i = 0):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.co.uk/search?q=articles+for+\" + query + \"&start=\" + str(i * 10))\n",
    "\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', 'https://google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6d7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(link):\n",
    "    \n",
    "    article_url = link.split(\"#\")[0]\n",
    "    article_content = article_url\n",
    "    \n",
    "    # Sending HTTP request\n",
    "    req = requests.get(article_url)\n",
    "\n",
    "    # Pulling HTTP data from internet\n",
    "    article_soup = BeautifulSoup(req.text, \"html.parser\") \n",
    "\n",
    "    # Pull Title\n",
    "    article_content += '\\n\\n' + article_soup.find_all('title')[0].text + '\\n\\n' \n",
    "    print('\\nScraping Article: ' + article_soup.find_all('title')[0].text)\n",
    "    \n",
    "    # Pull Content\n",
    "    for item in article_soup.find_all('body'):\n",
    "        for content in item.find_all(['li','p', 'h2']):\n",
    "            article_content += content.text + '\\n\\n'\n",
    "            \n",
    "    return article_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a246dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Inputs\n",
    "keywords = []\n",
    "with open('keywords.txt', 'r') as f:\n",
    "    keywords.append(f.readlines())\n",
    "    \n",
    "query_key = ' '\n",
    "for word in keywords[0]:\n",
    "    query_key += word + ' '\n",
    "\n",
    "query_key = query_key.replace('\\n', '')\n",
    "\n",
    "with open('input_params.txt', 'r') as f:\n",
    "    params = f.readlines()\n",
    "\n",
    "min_word_count = int(params[0])\n",
    "page = int(params[1]) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0799fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test A\n",
      "Test C\n",
      "\n",
      "Scraping Article: \"Unusual traffic from your computer network\" - Google Search Help\n",
      "Test B\n",
      "SKIP: Article is below the minimum word count of 800\n",
      "\n",
      " Scraping Done!\n"
     ]
    }
   ],
   "source": [
    "for index, link in enumerate(scrape_google(query_key, page)[:7]):\n",
    "\n",
    "    article_content = scrape_article(link)\n",
    "    \n",
    "    if len(article_content.split()) < min_word_count:\n",
    "        print('SKIP: Article is below the minimum word count of ' + str(min_word_count))\n",
    "        continue\n",
    "        \n",
    "    CHECK_FOLDER = os.path.isdir('output')\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.mkdir('output')\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    out = []\n",
    "    out.append(article_content)\n",
    "    out = np.array(out)\n",
    "\n",
    "\n",
    "    del article_content\n",
    "\n",
    "    np.savetxt('output/' + str(page) + \"_\" + str(index) + '.txt', out, fmt='%s', encoding=\"utf8\")\n",
    "    \n",
    "    print('Saved contents to ' + str(page) + \"_\" + str(index) + '.txt')\n",
    "\n",
    "    del out\n",
    "    \n",
    "print(\"\\n Scraping Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a846c951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef2e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
